{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Installing Required Libraries\n",
        "\n",
        "In this cell, we are installing the necessary Python libraries using the pip package manager.\n",
        "The libraries being installed are:\n",
        "1. transformers: A popular library for working with state-of-the-art natural language processing models.\n",
        "2. datasets: A library for easily working with and loading various datasets for machine learning tasks.\n",
        "3. sentencepiece: A library for text tokenization, used in natural language processing tasks.\n",
        "4. accelerate: A library for distributed training and inference with PyTorch.\n",
        "\n",
        "Let's run this cell to ensure the required dependencies are installed.\n"
      ],
      "metadata": {
        "id": "6Xq8DiWJpa82"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0BXGdutJzPXn"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets sentencepiece accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Importing Libraries\n",
        "\n",
        "In this cell, we import the necessary modules and classes from the installed libraries."
      ],
      "metadata": {
        "id": "vyaSfraqqNnl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "import torch"
      ],
      "metadata": {
        "id": "cwptMxzgzin4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Loading Pre-trained Model and Tokenizer\n",
        "\n",
        " In this cell, we load a pre-trained T5 model and its corresponding tokenizer.\n",
        " - model_name: The name of the pre-trained model to be used, in this case, \"lmqg/t5-base-squad-qag\".\n",
        " - tokenizer: T5Tokenizer instance created from the pre-trained model.\n",
        " - model: T5ForConditionalGeneration instance loaded from the pre-trained model.\n"
      ],
      "metadata": {
        "id": "W4GZlbYQqsVX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"lmqg/t5-base-squad-qag\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "Civ5VPMLzlPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Dataset\n",
        "\n",
        "In this cell, we load a dataset for the Question Answering and Generation task using the load_dataset function.\n",
        " - dataset: A dataset object loaded from the \"lmqg/qag_squad\" dataset."
      ],
      "metadata": {
        "id": "LHHfr91_rhUM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and prepare dataset\n",
        "dataset = load_dataset(\"lmqg/qag_squad\")"
      ],
      "metadata": {
        "id": "eURTlCFwzqhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Preprocessing Function\n",
        "\n",
        "In this cell, we define a data preprocessing function, preprocess_data, to prepare the dataset for the T5 model.\n",
        " - preprocess_data function takes examples as input and tokenizes the input context and target (question-answer pair).\n",
        " - The input is tokenized using the T5 tokenizer with specified maximum lengths and padding.\n",
        " - Labels (target) are tokenized separately as they require a different tokenizer configuration.\n"
      ],
      "metadata": {
        "id": "INZ5aD6br7aN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(examples):\n",
        "    # Preprocess input (context) and target (question and answer pair) for the model\n",
        "    model_inputs = tokenizer(examples['paragraph'], max_length=512, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    # Set up the tokenizer for targets\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(examples['questions_answers'], max_length=128, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_datasets = dataset.map(preprocess_data, batched=True)"
      ],
      "metadata": {
        "id": "_cC-lrUWzxBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training Configuration\n",
        "\n",
        "In this cell, we define the training arguments for configuring the training process.\n",
        " - TrainingArguments is a class that holds all the hyperparameters for training the model.\n",
        " - We specify the output directory for saving model checkpoints, evaluation strategy, learning rate, batch sizes for training and evaluation, number of training epochs, and weight decay."
      ],
      "metadata": {
        "id": "rUrqB1UpsQbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",             # Output directory for model checkpoints\n",
        "    evaluation_strategy=\"epoch\",        # Evaluate each epoch\n",
        "    learning_rate=2e-5,                 # Learning rate\n",
        "    per_device_train_batch_size=8,      # Batch size per device during training\n",
        "    per_device_eval_batch_size=8,       # Batch size for evaluation\n",
        "    num_train_epochs=3,                 # Number of training epochs\n",
        "    weight_decay=0.01,                  # Weight decay\n",
        ")"
      ],
      "metadata": {
        "id": "CW6CqdC80sv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model Training\n",
        "\n",
        "In this cell, we set up the Trainer class for training the T5 model.\n",
        " - Trainer is responsible for managing the training process, including optimization, logging, and evaluation.\n",
        " - We pass the pre-trained T5 model, training arguments, and tokenized training and validation datasets to the Trainer.\n"
      ],
      "metadata": {
        "id": "qWIJGOjvspbR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        ")"
      ],
      "metadata": {
        "id": "Ofr2nLn90wR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model Training Execution"
      ],
      "metadata": {
        "id": "1c0rDTvzs0ZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "e18-h7tA1GWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Now save the model."
      ],
      "metadata": {
        "id": "kefqxkUZtSmD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Importing NLTK and Transformers Libraries\n",
        "\n",
        "In this cell, we import the NLTK library for natural language processing tasks, download the necessary NLTK data, and import modules from the Transformers library.\n"
      ],
      "metadata": {
        "id": "dKaFpbM2_Whk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import json\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Import modules from the Transformers library\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import random"
      ],
      "metadata": {
        "id": "BVUw2QzjqQAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Loading Pre-trained Question Generation Model\n",
        "\n",
        "In this cell, we load a pre-trained question generation model and its corresponding tokenizer.\n",
        " - tokenizer: AutoTokenizer instance created from the pre-trained model \"abhir00p/Qgen_model_ACM\".\n",
        " - model: AutoModelForSeq2SeqLM instance loaded from the pre-trained model."
      ],
      "metadata": {
        "id": "Ept8eW96_piG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"abhir00p/Qgen_model_ACM\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"abhir00p/Qgen_model_ACM\")"
      ],
      "metadata": {
        "id": "ZCSxOcJsqQpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Function for Generating Question-Answer Pairs\n",
        "\n",
        "In this cell, we define a function, generate_qa_pairs, that takes a context and generates question-answer pairs using a pre-trained model.\n",
        " - context: The input text from which questions are generated.\n",
        " - model: The pre-trained question generation model.\n",
        " - tokenizer: The tokenizer corresponding to the model.\n",
        " - num_pairs: The number of question-answer pairs to generate (default is 1).\n",
        " - max_length: The maximum length of the input text chunk for processing."
      ],
      "metadata": {
        "id": "dnbQYRDIAKa_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_qa_pairs(context, model, tokenizer, num_pairs=1, max_length=256):\n",
        "    qa_pairs = []\n",
        "\n",
        "    current_start = 0\n",
        "    while current_start < len(context):\n",
        "        current_end = min(current_start + max_length, len(context))\n",
        "        chunk = context[current_start:current_end]\n",
        "\n",
        "        encoding = tokenizer.encode_plus(chunk, max_length=max_length, truncation=True, return_tensors=\"pt\")\n",
        "        input_ids, attention_mask = encoding[\"input_ids\"], encoding[\"attention_mask\"]\n",
        "\n",
        "        output_sequences = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            early_stopping=True,\n",
        "            num_beams=3,\n",
        "            num_return_sequences=num_pairs,\n",
        "            no_repeat_ngram_size=2,\n",
        "            max_length=200,\n",
        "        )\n",
        "\n",
        "        for sequence in output_sequences:\n",
        "            qa_text = tokenizer.decode(sequence, skip_special_tokens=True)\n",
        "            qa_parts = qa_text.split(\"[SEP]\")\n",
        "            qa_pairs.append(qa_text)\n",
        "\n",
        "\n",
        "        current_start = current_end\n",
        "\n",
        "    return qa_pairs\n"
      ],
      "metadata": {
        "id": "sMLPr1vGTraJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"\"\"Introduction:\n",
        "\n",
        "Constrained Application Protocol resulted from Internet Engineering Task Force (IETF) Constrained RESTful Environments Request For Comments (CORE - RFC) working group's efforts to develop a generic framework for resource- oriented applications targeting constrained nodes & networks.\n",
        "\n",
        "COAP framework (RFC 7252) defines simple & flexible ways to manipulate sensors & actuators for data or device management.\n",
        "\n",
        "COAP messaging model is primarily designed to facilitate the exchange of messages over UDP between endpoints, including secure transport protocol Datagram Transport Layer Security (DTLS).\n",
        "BECE351E-EK\n",
        "\n",
        "COAP\n",
        "\n",
        "5/17 100% +\n",
        "\n",
        "73\n",
        "\n",
        "Introduction:\n",
        "\n",
        "• COAP is based on request/response communication model\n",
        "\n",
        "similar to HTTP & supports additional protocol features that are\n",
        "\n",
        "useful in loT scenarios.\n",
        "\n",
        "Popular use case: Wired & wireless sensor networks.\n",
        "\n",
        "Due to its frequent usage in constrained & local networks, CoAP is more suitable for Internet wide data transfer over HTTP.\n",
        "\n",
        "CoAP can efficiently work on constrained devices, even when these devices are connected to highly lossy networks with high packet loss, high error rates & bandwidth in range of kilobits.\n",
        "\n",
        "Highlights: Service discovery, resource discovery, URIS (uniform resource identifier), Internet media handling support, easy HTTP integration & multicasting while maintaining low overheads.\n",
        "Introduction:\n",
        "\n",
        "6/17- 100% +\n",
        "\n",
        "• CoAP implementations can act as both clients & servers (not simultaneously).\n",
        "\n",
        "COAP client's request signifies a request for action from an identified resource on a server, which is similar to HTTP.\n",
        "\n",
        "• Response sent by the server in the form of a response code can contain resource representations as well.\n",
        "\n",
        "Interchanges are asynchronous & datagram-oriented over UDP.\n",
        "\n",
        "• Packet collisions are handled by a logical message layer incorporating the exponential backoff mechanism for providing reliability.\n",
        "\n",
        "Two distinct layers of messaging (which handle UDP & asynchronous messaging) and request-response (which handles connection establishment) are part of CoAP header.\n",
        "UDP as Transport Protocol:\n",
        "\n",
        "• CoAP is based on UDP, which has a variety of unique features while being slim & efficient, yet not always ideal for Internet communication or communication between multiple networks due to its non-reliable nature.\n",
        "\n",
        "CoAP implements simple mechanisms to mitigate these issues:\n",
        "\n",
        "Simple stop-and-wait retransmission: CoAP message can be marked as a confirmable message by adding a protocol flag.\n",
        "\n",
        "Deduplication: CoAP has a built-in deduplication mechanism based on message identifiers. This mechanism is in place for all CoAP messages.\n",
        "\n",
        "CoAP is capable of using other transport protocols like TCP or SMS.\n",
        "COAP\n",
        "\n",
        "Message Format:\n",
        "\n",
        "COAP message is composed of a short fixed-length header field (4 bytes), a variable-length but mandatory token field (0-8 bytes), options fields if necessary & payload field.\n",
        "\n",
        "CoAP message delivers low overhead while decreasing parsing complexity.\n",
        "COAP\n",
        "\n",
        "Functionality:\n",
        "\n",
        "• COAP can run over IPv4 or IPv6, it is recommended that message fit within a single IP packet & UDP payload to avoid fragmentation.\n",
        "\n",
        "For IPv6, with the default MTU size being 1280 bytes & allowing for no fragmentation across nodes, maximum CoAP message size could be up to 1152 bytes, including 1024 bytes for the payload.\n",
        "\n",
        "For IPv4, as IP fragmentation may exist across network, implementations should limit themselves to more conservative values & set IPv4 Don't Fragment (DF) bit.\n",
        "\n",
        "While most sensor & actuator traffic utilizes small-packet payloads, some use cases, such as firmware upgrades, require capability to send larger payloads.\n",
        "\n",
        "COAP doesn't rely on IP fragmentation but defines a pair of Block options for transferring multiple blocks of information from a resource representation in multiple request/response pairs.\n",
        "Functionality:\n",
        "\n",
        "• Like HTTP, CoAP is based on the REST architecture, but with a \"thing\" acting as both client & server.\n",
        "\n",
        "• Through the exchange of asynchronous messages, a client requests an action via a method code on a server resource.\n",
        "\n",
        "• Uniform resource identifier (URI) localized on server identifies this resource and responds back.\n",
        "\n",
        "COAP request/response semantics include methods GET, POST, PUT & DELETE.\n",
        "COAP\n",
        "\n",
        "Reliable Transmission:\n",
        "\n",
        "COAP defines four types of messages: confirmable, non- confirmable, acknowledgement & reset.\n",
        "\n",
        "While running over UDP, CoAP offers a reliable transmission of messages when a CoAP header is marked as \"confirmable.\"\n",
        "\n",
        "If a request or response is tagged as confirmable, recipient must explicitly either acknowledge or reject.\"\"\""
      ],
      "metadata": {
        "id": "ezRFrZi67_7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_pairs = generate_qa_pairs(context, model, tokenizer)"
      ],
      "metadata": {
        "id": "7OJeMe_CYGBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_pairs"
      ],
      "metadata": {
        "id": "WyQcn0-krCOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Function for Cleaning Question-Answer Pairs\n",
        "\n",
        "In this cell, we define a function, clean_qa_pairs, to clean the generated question-answer pairs.\n",
        "The function extracts and formats the question and answer from each generated pair.\n"
      ],
      "metadata": {
        "id": "x1SkXobAAbbP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_qa_pairs(qa_pairs):\n",
        "    cleaned_qa_pairs = []\n",
        "    for pair in qa_pairs:\n",
        "        parts = pair.split(\", answer: \", 1)\n",
        "        if len(parts) > 1:\n",
        "            question = parts[0].split(\"question: \")[1]\n",
        "            answer = parts[1].split(\"|\")[0].capitalize()\n",
        "            cleaned_qa_pairs.append({\"question\": question, \"answer\": answer})\n",
        "    return cleaned_qa_pairs"
      ],
      "metadata": {
        "id": "BHON0nWnstBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_pairs = clean_qa_pairs(qa_pairs)"
      ],
      "metadata": {
        "id": "mQtqRjyFYwMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_pairs"
      ],
      "metadata": {
        "id": "KM447Fjza6SY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#conversion to a csv\n",
        "import pandas as pd\n",
        "def conversiontocsv(cleaned_pairs):\n",
        "  df=pd.read_json(str({\"data\":cleaned_qa_pairs}))\n",
        "  csv_file_download=df.to_csv(\"csv_cleaned_qa_pairs.csv\")\n",
        "  return csv_file_download"
      ],
      "metadata": {
        "id": "EaaqvB7gmVqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using LMQG Package for pipeline function"
      ],
      "metadata": {
        "id": "_FOyFdzrKmfN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Installing lmqg and Using TransformersQG for Question Generation\n",
        "\n",
        "In this cell, we install the lmqg library using the pip package manager.\n",
        "Then, we use the TransformersQG class to generate question-answer pairs from a given context.\n"
      ],
      "metadata": {
        "id": "y0IrhqUQRX58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install lmqg"
      ],
      "metadata": {
        "id": "wrIikpNSKbfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from lmqg import TransformersQG\n",
        "\n",
        "# initialize model\n",
        "model = TransformersQG(model=\"abhir00p/Qgen_model_ACM\")\n"
      ],
      "metadata": {
        "id": "SbqeOX90K7bV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model prediction\n",
        "question_answer_pairs = model.generate_qa(context)"
      ],
      "metadata": {
        "id": "pd7rQjl7KxwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Inference API"
      ],
      "metadata": {
        "id": "OJuj23m-tXGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install langchain library\n",
        "!pip install langchain"
      ],
      "metadata": {
        "id": "3pDsiFF94ba8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter"
      ],
      "metadata": {
        "id": "-aMPjU6h4YPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Text Chunking\n",
        "\n",
        " In this cell, we define a function, get_text_chunks, to split a given text into chunks using langchain's CharacterTextSplitter.\n",
        " The function takes a text as input, splits it into chunks, and removes newline characters from each chunk."
      ],
      "metadata": {
        "id": "lwEkKN6pSG8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_text_chunks(text):\n",
        "    # Initialize CharacterTextSplitter with specified parameters\n",
        "    text_splitter = CharacterTextSplitter(\n",
        "        separator=\"\\n\",\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=200,\n",
        "        length_function=len\n",
        "    )\n",
        "\n",
        "    # Split the text into main_chunks\n",
        "    main_chunks = text_splitter.split_text(text)\n",
        "\n",
        "    # Remove newline characters from each chunk\n",
        "    for chunk in range(0, len(main_chunks)):\n",
        "        main_chunks[chunk] = main_chunks[chunk].replace('\\n', \"\")\n",
        "\n",
        "    return main_chunks"
      ],
      "metadata": {
        "id": "izBrTNmr4Vpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"\"\"\n",
        "Toy Story 4 is a 2019 American animated comedy-drama film produced by Pixar Animation Studios for Walt Disney Pictures. It is the fourth installment in Pixar's Toy Story series and the sequel to Toy Story 3 (2010). It was directed by Josh Cooley (in his feature directorial debut) from a screenplay by Andrew Stanton and Stephany Folsom; the three also conceived the story alongside John Lasseter, Rashida Jones, Will McCormack, Valerie LaPointe, and Martin Hynes.[5] Tom Hanks, Tim Allen, Annie Potts, Joan Cusack, Don Rickles,[b] Wallace Shawn, John Ratzenberger, Estelle Harris (in her final film role), Blake Clark, Jeff Pidgeon, Bonnie Hunt, Jeff Garlin, Kristen Schaal, and Timothy Dalton reprise their character roles from the first three films, and are joined by Tony Hale, Keegan-Michael Key, Jordan Peele, Christina Hendricks, Keanu Reeves, and Ally Maki, who voice new characters introduced in this film. Set after the third film, Toy Story 4 follows Woody (Hanks) and Buzz Lightyear (Allen) as the pair and the other toys go on a road trip with Bonnie (Madeleine McGraw), who creates Forky (Hale), a spork made with recycled materials from her school. Meanwhile, Woody is reunited with Bo Peep (Potts), and must decide where his loyalties lie.\n",
        "\n",
        "Talks for a fourth film began in 2010, and Hanks stated that Pixar was working on the sequel in 2011. When the film was officially announced in November 2014 during an investor's call, it was reported that the film would be directed by Lasseter, who later announced it would be a love story, after writing a film treatment with Stanton, and input from Pete Docter and Lee Unkrich, while Galyn Susman would serve as the producer. Cooley became the film's co-director in March 2015, while Pixar president Jim Morris said it was not a continuation of the third film, who described the film as a romantic comedy. In July 2017, Lasseter was stepping down and leaving Cooley as the sole director. Despite this, Lasseter still retained writing credits. New characters for the film were announced in 2018 and 2019 along with new cast members. Composer Randy Newman returned to score the film, marking his ninth collaboration with Pixar. The film is dedicated to Don Rickles (the voice of Mr. Potato Head) and animator Adam Burke, who died in 2017 and 2018, respectively.[6][7]\n",
        "\n",
        "Toy Story 4 premiered in Hollywood, Los Angeles, on June 11, 2019, and was released in the United States on June 21. It grossed $1.073 billion worldwide, becoming the eighth-highest-grossing film of 2019 and is the highest-grossing film in the franchise, marginally surpassing Toy Story 3. Like its predecessors, the film received acclaim from critics, with praise for its story, humor, emotional depth, musical score, animation, and vocal performances. The film was nominated for two awards at the 92nd Academy Awards, winning Best Animated Feature, and received numerous other accolades. Whilst Toy Story 4 film was initially expected to be the final film in the main Toy Story film series a fifth installment is in development.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "pz1GThrw4wO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_chunks = get_text_chunks(context)"
      ],
      "metadata": {
        "id": "hh6TYNAu42gE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question Generation via Hugging Face Inference API\n",
        "\n",
        " In this cell, we define a function, generate_qa_pairs, to generate question-answer pairs using the Hugging Face Inference API.\n",
        " The function takes a context, model_name, API token, and other optional parameters for generating QA pairs."
      ],
      "metadata": {
        "id": "EX6qLy_fSd_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# API_URL = \"https://api-inference.huggingface.co/models/abhir00p/Qgen_model_ACM\"\n",
        "# headers = {\"Authorization\": \"Bearer your_ACCESS_TOKEN\"}\n"
      ],
      "metadata": {
        "id": "U4Ff6C3DtZGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_qa_pairs(context, model_name, api_token, num_pairs=1, max_length=256):\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {api_token}\"\n",
        "    }\n",
        "\n",
        "    qa_pairs = []\n",
        "    current_start = 0\n",
        "    while current_start < len(context):\n",
        "        current_end = min(current_start + max_length, len(context))\n",
        "        chunk = context[current_start:current_end]\n",
        "\n",
        "        payload = {\n",
        "            \"inputs\": chunk,\n",
        "            \"parameters\": {\n",
        "                \"max_length\": 200,\n",
        "                \"num_beams\": 3,\n",
        "                \"num_return_sequences\": num_pairs,\n",
        "                \"no_repeat_ngram_size\": 2\n",
        "            },\n",
        "            \"options\": {\n",
        "                \"use_cache\": False,\n",
        "                \"wait_for_model\": True\n",
        "            }\n",
        "        }\n",
        "\n",
        "        response = requests.post(f\"https://api-inference.huggingface.co/models/{model_name}\", headers=headers, json=payload)\n",
        "        if response.status_code == 200:\n",
        "            output_sequences = response.json()\n",
        "            for sequence in output_sequences:\n",
        "                qa_text = sequence[\"generated_text\"]\n",
        "                # Optionally split the text into Q&A parts\n",
        "                # qa_parts = qa_text.split(\"[SEP]\")\n",
        "                qa_pairs.append(qa_text)\n",
        "\n",
        "    current_start = current_end\n",
        "\n",
        "    return qa_pairs\n",
        "\n"
      ],
      "metadata": {
        "id": "6s2EB0m7wcV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_qa_pairs(context_chunks,\"abhir00p/Qgen_model_ACM\",\"your_ACCESS_TOKEN\")"
      ],
      "metadata": {
        "id": "Cm8-l5LXuyvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "References:\n",
        " 1. https://github.com/asahi417/lm-question-generation?tab=readme-ov-file\n",
        " 2. https://huggingface.co/learn/nlp-course/chapter1/1\n",
        " 3. https://huggingface.co/lmqg\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yZflW1tNpZXd"
      }
    }
  ]
}